{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeuHJ4dUysfz"
   },
   "source": [
    "#                                       *Climate* *Change* *Sentiment* *Analysis*\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7ziSF2ri8MH"
   },
   "source": [
    "Add a theme image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dd2OJWLzmcni"
   },
   "source": [
    "***Table of Contents***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AB8d3qSImimX"
   },
   "source": [
    "Introduction\n",
    "\n",
    "Data Overview\n",
    "\n",
    "Installed Libraries\n",
    "\n",
    "Import Libraries \n",
    "\n",
    "1. Importing and viewing the Datasets\n",
    "\n",
    "2. Basic Data Analysis\n",
    "  \n",
    "\n",
    "3. Preprocessing\n",
    "  \n",
    "  *Create copy\n",
    "  \n",
    "  *Cleaning the data \n",
    "\n",
    "4. Explanatory Data Analysis\n",
    "\n",
    " \n",
    "  * Find Entities\n",
    "  * Lemmatization\n",
    "  * Sentiment distribution\n",
    "  * Tweet distribution\n",
    "  * Climate change popular words\n",
    "  * Climate change popular hashtags\n",
    "  * Climate change popular named entities\n",
    "  * News bulletins\n",
    "  * People and places\n",
    "\n",
    "5. Machine Learning\n",
    "\n",
    "  5.1. Train-validation split\n",
    "\n",
    "  5.2. Train models\n",
    "\n",
    "6. Model evaluation\n",
    "\n",
    "  6.1. Modeling\n",
    "\n",
    "  * Model #1 \n",
    "\n",
    "  * Model #2\n",
    "\n",
    "  * Model #3\n",
    "\n",
    "  * Model #4\n",
    "\n",
    "  * Model #5\n",
    "\n",
    "  6.2. Model Selection\n",
    "\n",
    "  * Models score comparison\n",
    "\n",
    "  * Hyperparameter tuning\n",
    "\n",
    "  6.3. Model Improvement(If any)\n",
    "\n",
    "Submission\n",
    "\n",
    "Conclusion\n",
    "\n",
    "Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6oFAvBV8aRG"
   },
   "source": [
    "#**Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyNU7wGSizZM"
   },
   "source": [
    "It is undeniable that climate change is one of the most talked topics of our times and one of the biggest challenges the world is facing today. In the past few years, we have seen a steep rise on the Earth's temperature, causing a spike in wild fires, drought, rise of sea levels due to melting glaciers, rainfall pattern shifts, flood disasters. \n",
    "\n",
    "While this may be true to most around the world, for one or many reasons, there are still some who are yet unphased or better yet indenial of dangers imposed by climate channge. With that being said, it is of a compelling matter to the public and its governments to engage in such a dialogue. \n",
    "\n",
    "Here is a document tasked to share an insight on the global climate change discussion based on data collected from Twitter. In this document we will perform a sentiment on climate change analysis to help draw an idea of what pun=blic thinks through the use of data computation and machine learning.\n",
    "\n",
    "See below for more discription on the data and outline of this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31N0XCsOifFu"
   },
   "source": [
    "*   **Data overview**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLHuwMvqiiZc"
   },
   "source": [
    "> The collection of this data was funded by a Canada Foundation for Innovation JELF Grant to Chris Bauch, University of Waterloo. The dataset aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018. In total, 43943 tweets were collected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-AH0YjMQJcE"
   },
   "source": [
    "*   **Outline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RtM-LVnQS6T"
   },
   "source": [
    "> In this document we will perform data analysis, build a machine learning classifier model that will be used to predict tweet sentiments based on tweets using Python programming language. \n",
    "\n",
    "> The following list sums up the outline of this document:\n",
    "*  Basic data analysis\n",
    "* Clean data\n",
    "* Perform an explanatory data analysis\n",
    "* Tokenize and lemmatize cleaned data\n",
    "* Build models\n",
    "* Perform hyperparameter tuning\n",
    "* Resample data\n",
    "* Perform predictions on unseen data\n",
    "\n",
    "> It is also important to not that a Streamlit App based on this model will be built. The app will facilitate as a user-friendly plartform for users to perform sentiment predictions based on tweets and use as an on-hand climate change informative tool for users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2T6brWqO1ZkS"
   },
   "source": [
    "#**Install NLP Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wruWGnu1ZkT"
   },
   "source": [
    "The following special libraries were installed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agQ7LkTh0Z2q"
   },
   "source": [
    "**spacy** is designed specifically for production use and helps build applications that process and “understand” large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.\n",
    "\n",
    "The **Natural Language Toolkit** is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Nx6wBtTChk5U"
   },
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !pip install NLTK\n",
    "# !pip install comet_ml\n",
    "# !conda install -c conda-forge imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "752ir0eG1BUd"
   },
   "source": [
    "#**Importing the required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_vG_T9DtyqJe"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'delayed' from 'sklearn.utils.fixes' (C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e639c299c1ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBernoulliNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExtraTreesClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseEnsemble\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_forest\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_forest\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_forest\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomTreesEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m from ..tree import (DecisionTreeClassifier, DecisionTreeRegressor,\n\u001b[0m\u001b[0;32m     57\u001b[0m                     ExtraTreeClassifier, ExtraTreeRegressor)\n\u001b[0;32m     58\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDOUBLE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \"\"\"\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_classes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseDecisionTree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_classes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_classes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_deprecate_positional_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_criterion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCriterion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_splitter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSplitter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_tree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDepthFirstTreeBuilder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_criterion.pyx\u001b[0m in \u001b[0;36minit sklearn.tree._criterion\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_splitter.pyx\u001b[0m in \u001b[0;36minit sklearn.tree._splitter\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36minit sklearn.tree._tree\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_kd_tree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKDTree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_dist_metrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDistanceMetric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkneighbors_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mradius_neighbors_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNeighborsTransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRadiusNeighborsTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_unsupervised\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_graph.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# License: BSD 3 clause (C) INRIA, University of Amsterdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNeighborsMixin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRadiusNeighborsMixin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNeighborsBase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_unsupervised\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_non_negative\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEfficiencyWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'delayed' from 'sklearn.utils.fixes' (C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import rcParams\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from numpy import arange\n",
    "import re           # \"re\", regular expression module included with Python primarily used for string searching and manipulation\n",
    "                    # Also used frequently for web page \"Scraping\" (extract large amount of data from websites)\n",
    "import os\n",
    "import re, string\n",
    "import spacy.cli\n",
    "import nltk         # The Natural Language Toolkit\n",
    "\n",
    "#Language libraries for text cleaning\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "#Resampling libraries\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#Machine Learning Models\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "# Packages for training models\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import  KFold\n",
    "\n",
    "#Matrix measurement\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Model Evaluation Packages\n",
    "from sklearn.metrics import accuracy_score, log_loss, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "#from sklearn.metrics import recall_score, precision_recall_curve,\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3Qrj4YjijW6"
   },
   "outputs": [],
   "source": [
    "nltk.download('words')\n",
    "spacy.cli.download('en_core_web_sm')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-loe_yeFYc2D"
   },
   "outputs": [],
   "source": [
    "stop = nltk.corpus.stopwords.words('english')\n",
    "words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iU69KhC5__vy"
   },
   "source": [
    "#**1. Data sets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJlh_PGlCMbF"
   },
   "source": [
    "*   **Importing datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2ZRcRRWjCn9"
   },
   "outputs": [],
   "source": [
    "# Importing test datasets\n",
    "test = pd.read_csv('https://raw.githubusercontent.com/ThobaniMwandla/advanced_classification_zm6_project/main/data/test_with_no_labels.csv')\n",
    "# Importing train datasets\n",
    "train= pd.read_csv('https://raw.githubusercontent.com/ThobaniMwandla/advanced_classification_zm6_project/main/data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdvhRQG6WXtB"
   },
   "source": [
    "*   **View datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYMLQNfdYB44"
   },
   "source": [
    "> View the first five rows of the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "u5BE-TIX1Zkg",
    "outputId": "99d8712c-b233-4694-d97b-76eea4fc1a39"
   },
   "outputs": [],
   "source": [
    "# View train data\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXkRnUWheI2p"
   },
   "source": [
    "\n",
    "\n",
    "> As displayed above, this data contains a lot of noise. The hashtags, @-metions, uppercase, numbers, etc, creates a lot of noise that might be distractive to our model. Therefore, this will need to be cleaned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTkrO8VbYcVt"
   },
   "source": [
    "> View the first five rows of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "rDteItN71Zkh",
    "outputId": "4a40fdb4-8ed6-4be6-b7dd-07e0b64e8d7a"
   },
   "outputs": [],
   "source": [
    "# View test data\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEJFVgBRe1Xm"
   },
   "source": [
    "\n",
    "\n",
    "> The same level of noise is demonstrated in the test data. Therefore, it must also be cleaned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjJmKNotNBmv"
   },
   "source": [
    "#**2. Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGfqcyFMY24V"
   },
   "source": [
    "\n",
    "\n",
    "> Here we are preprocessing the data by first performing basic data checks such as data shape, anomalies, missing values, duplicates, datatypes, and then clean the text by removing the noise and stopwords. \n",
    "\n",
    "> After cleaning the data, we then tokenize and lemmatize the data text for to convert all the individual words on the text to base their base form so they can easily be grouped together and analysed as a single item.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwL1rJvgAf6A"
   },
   "source": [
    "##2.1. Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xSIhR4VXfWx"
   },
   "source": [
    "* **Create copies**\n",
    "\n",
    "> We create copies of raw data for later use on EDA. This will be used for text extractions such as hashtags and tweet handles extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqB-zOiuK70q"
   },
   "outputs": [],
   "source": [
    "# Preserve raw data by copying for later use\n",
    "eda_train = train.copy()\n",
    "eda_test = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a6DUPhe1Zkk"
   },
   "source": [
    "\n",
    "> As shown above, there are 15,819 rows and 3 columns in the train data; And 10,546 rows and 2 columns in the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHvWMP4CHkES"
   },
   "source": [
    "\n",
    "\n",
    "*   ***Datatype check***\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eYunhru1Zkm"
   },
   "source": [
    "> Checking and exploring the DataTypes of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qONSvvYv1Zkm",
    "outputId": "5772c294-aea2-483d-ced2-f00dcd2880d7"
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRY0pNTMcKqS"
   },
   "source": [
    "\n",
    "\n",
    "> As shown above, there are 15,819 rows and 3 columns (sentiment of int type, message of object type, tweetid of int type) in the train data;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDuvbPAf1Zkn",
    "outputId": "0a30535d-2558-4e3c-c429-c53b50b8d103"
   },
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnXHUjYJcnWP"
   },
   "source": [
    "\n",
    "\n",
    "> And 10,546 rows and 2 columns (message of object type, tweetid of int type) in the test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH9EQtOhClBs"
   },
   "source": [
    "##2.2. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMX0DkNZHKdS"
   },
   "source": [
    "\n",
    "\n",
    "*   ***Missing values***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkmZTmenYzNh"
   },
   "source": [
    "\n",
    "> Check for missing values, if any, replace with random.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUdKXeHC1Zkk",
    "outputId": "f3b287c5-ee72-4eb2-9c7a-2fc58730a03a"
   },
   "outputs": [],
   "source": [
    "print(train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tFOsNfF1Zkl",
    "outputId": "d12ebd3b-cfd8-4634-e2c9-447456d9aea1"
   },
   "outputs": [],
   "source": [
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYvrkzms1Zkl"
   },
   "source": [
    "> This confirms that there are no nulls in both the test and train datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsYfgG1kH40S"
   },
   "source": [
    "\n",
    "\n",
    "*   ***Check for duplicates***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIDeXj7DJL7I"
   },
   "source": [
    "\n",
    "*   ***Noise removal***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7A7Hx3EK8OxP"
   },
   "source": [
    "> After going through the data, it was noticed that there was noise on the 'message' column containing the tweets. \n",
    "\n",
    "> Therefore, the noise is removed from the tweets firstly by applying the lower-case function to all entries, and then remove urls, twitter handles with @-mentions, hashtags, white spaces, numbers and punctuation marks.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FT0mtEVjZv2"
   },
   "outputs": [],
   "source": [
    "def clean_text(df):\n",
    "  item = 0\n",
    "  for tweet in df['message']:\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    tweet = re.sub(r'@\\S+', '', tweet)\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "    tweet = tweet.lstrip()\n",
    "    tweet = tweet.rstrip()\n",
    "    tweet = tweet.replace('  ', ' ')\n",
    "\n",
    "    tweet = re.sub(r'[-]',' ', tweet)\n",
    "    tweet = re.sub(r'[^\\w\\s]','', tweet)\n",
    "    tweet = re.sub('[0-9]+', '', tweet)\n",
    "\n",
    "    df.loc[item, 'message'] = tweet\n",
    "    item += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_KjLK_Qhahs"
   },
   "source": [
    "\n",
    "\n",
    "> Apply the cleaning function on the train data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "imNo7YM9PDvu",
    "outputId": "bf131717-56b3-4b2b-a181-5d8bcd989f8a"
   },
   "outputs": [],
   "source": [
    "clean_text(train)\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXTCkgBmhlyM"
   },
   "source": [
    "> Apply the cleaning function on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "XZGhAPi_TIl6",
    "outputId": "42f14ba8-c8cf-4d3e-e96a-a7eabde61839"
   },
   "outputs": [],
   "source": [
    "clean_text(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGiWpv04215t"
   },
   "source": [
    "\n",
    "*   ***Stopwords removal***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRmsB4hjh2qO"
   },
   "source": [
    "> Stopwords are the English words which do not contribute much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have, is, they, are stopwords.\n",
    "\n",
    ">Stopwords removal is very important when dealing with Natural Processing Language or text classification problems especially where the text is to be classified to certain categories, in this case, sentiments. \n",
    "\n",
    "> Hence, stopwords are excluded from the tweets in the 'messge' column so that more focus can be given to those words which define the meaning of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRaM34T3jkNJ"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(df):\n",
    "    new_stopwords = stopwords.words('english')\n",
    "    row = 0\n",
    "    for tweet in df['message']:\n",
    "      tweet = word_tokenize(tweet)\n",
    "      tweet = [word for word in tweet if not word in new_stopwords]\n",
    "      tweet = ' '.join(tweet)\n",
    "\n",
    "      df.loc[row, 'message'] = tweet\n",
    "      row += 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3zWwDSnjXth"
   },
   "source": [
    "> Remove stopwords from train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "-k21QOvEBoUi",
    "outputId": "ddd80601-891f-4989-fa17-41d3a70a7b5e"
   },
   "outputs": [],
   "source": [
    "remove_stopwords(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8S0IX9x5jh96"
   },
   "source": [
    "> Stopwords removed from test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "E2Szdx35TM4F",
    "outputId": "1bbb2962-44f6-4481-8ae2-0ed6911a090d"
   },
   "outputs": [],
   "source": [
    "remove_stopwords(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7MYaT_ykf1b"
   },
   "source": [
    "> As shown on the datframe above, words like 'is', 'are' have now been ommitted from the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCbgGYy83POz"
   },
   "source": [
    "*   ***Word Normalization***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsvdDXhyk_NG"
   },
   "source": [
    "> Above as mentioned, we sre more driving by the idea that more focus should be given to those words which define the meaning of the tweet for the model to be able to train more effectively.\n",
    "\n",
    "> This brings us to what is referred to as word or text normalization. Text normalization allows us to not only reduce words to their base form but also group words of the same meaning according to the given context. This helps sharpens the classifier even more in effectively recognizing not only the context of text but also patterns, effectively improving its ability to classify categories. For example, lemmatization of of the word 'better' will output the word 'good', which helps the classifier not see the word 'better' as a different 'thing' from the word 'good', and this is one of the ways machines 'understand' English, or language.\n",
    "\n",
    "> Here we used the Spacy 'en_core_web_sm' package to lemmatize each and every tweet by iterating through the tokenized rows in the 'message' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A490Wi48k_gs"
   },
   "outputs": [],
   "source": [
    "def lemmatize_tweet(df):\n",
    "    df_index = 0\n",
    "\n",
    "    for tweet in df['message']:\n",
    "      tweet = nlp(tweet)\n",
    "      \n",
    "      for token in tweet:\n",
    "        df.loc[df_index, 'message'] = df.loc[df_index, 'message'].replace(str(token.text), str(token.lemma_))\n",
    "\n",
    "      df_index += 1\n",
    "\n",
    "      return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "LNMW3AsWRVAA",
    "outputId": "cb89ec65-4836-41cf-b947-3e66a85b0ceb"
   },
   "outputs": [],
   "source": [
    "lemmatize_tweet(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "7cj6gDObTccc",
    "outputId": "453b80b6-8120-4a45-ccb8-68e9e64f2e03"
   },
   "outputs": [],
   "source": [
    "lemmatize_tweet(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwG4Wo554lDJ"
   },
   "source": [
    "\n",
    "\n",
    "*   **Named entities**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQcmH8-M7WXH"
   },
   "source": [
    "\n",
    "> Similarly to word normalization, here we also attempt to convert words of named entities such as names of places and names of organisations, for example, to represantative or place-holder terms, such as 'LOC' and 'ORG', respectively, in this case. This further reduces noise for the classifier to effectively learn and recognise patterns. \n",
    "\n",
    "> We used the Spacy package to identify and replace entities in each and every tweet in the 'message' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HISNTJcj9tI"
   },
   "outputs": [],
   "source": [
    "def entities(df):\n",
    "    indx = 0\n",
    "\n",
    "    for tweet in df['message']:\n",
    "      tweet = nlp(tweet)\n",
    "\n",
    "      for entity in tweet.ents:\n",
    "        df.loc[indx, 'message'] = df.loc[indx, 'message'].replace(str(entity.text), str(entity.label_))\n",
    "\n",
    "      indx += 1\n",
    "\n",
    "      return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "ohas__j-QFGV",
    "outputId": "d01be04f-da52-41ba-f014-2a31c24d2329"
   },
   "outputs": [],
   "source": [
    "entities(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "lmL3Ad3rTZHC",
    "outputId": "591f07dd-3da1-4af7-de9c-1b244f6e7f17"
   },
   "outputs": [],
   "source": [
    "entities(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjfJWKpqUDbH"
   },
   "source": [
    "##**2.3. Visualize Distributions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDHN3hbv-3tr"
   },
   "source": [
    "* **Tweet distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ep7HHZ8N1Zki"
   },
   "source": [
    ">The tweets are divided into 4 sentiments/classes:\n",
    "\n",
    ">[ 0 ] Neutral : Tweets that neither support nor refuse beliefs of climate change.\n",
    "\n",
    ">[-1 ] Anti : Tweets that do not support the belief of man-made climate change.\n",
    "\n",
    ">[ 1 ] Pro : Tweets that support the belief of man-made climate change\n",
    "\n",
    ">[ 2 ] News : Tweets linked to factual news about climate change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSqfJTVt7yN-"
   },
   "outputs": [],
   "source": [
    "def update(df):\n",
    "    sentiment = df['sentiment']\n",
    "    word_sentiment = []\n",
    "\n",
    "    for i in sentiment :\n",
    "        if i == 1 :\n",
    "            word_sentiment.append('Pro')\n",
    "        elif i == 0 :\n",
    "            word_sentiment.append('Neutral')\n",
    "        elif i == -1 :\n",
    "            word_sentiment.append('Anti')\n",
    "        else :\n",
    "            word_sentiment.append('News')\n",
    "\n",
    "    df['sentiment'] = word_sentiment\n",
    "    \n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "m1YiYNj_KKj9",
    "outputId": "1a965e30-5011-49f9-b534-8c72a44f6df9"
   },
   "outputs": [],
   "source": [
    "eda_train = update(eda_train)\n",
    "eda_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDrbGvPtW2sz",
    "outputId": "2c76bd0f-9c6e-4eac-86b4-ba4c739497f5"
   },
   "outputs": [],
   "source": [
    "eda_train.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Qe0V_Fb_Dex"
   },
   "outputs": [],
   "source": [
    "import matplotlib.style as style \n",
    "sns.set(font_scale=1.5)\n",
    "#style.use('seaborn-pastel')\n",
    "#style.use('seaborn-poster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lUTSRQPi8nja",
    "outputId": "6a22ddb3-5580-495f-c0df-d4f411c3f127"
   },
   "outputs": [],
   "source": [
    "style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "sH3bBc0N1Zk7",
    "outputId": "24ad365e-3638-40d5-d519-2f7fdb999d34"
   },
   "outputs": [],
   "source": [
    "style.use('fivethirtyeight')\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, \n",
    "                         nrows=1, \n",
    "                         figsize=(15, 5), \n",
    "                         dpi=100)\n",
    "\n",
    "sns.countplot(eda_train['sentiment'], ax=axes[0])\n",
    "\n",
    "labels=['Pro', 'News', 'Neutral', 'Anti'] \n",
    "\n",
    "axes[1].pie(eda_train['sentiment'].value_counts(),\n",
    "            labels=labels,\n",
    "            autopct='%1.0f%%',\n",
    "            shadow=True,\n",
    "            startangle=90,\n",
    "            explode = (0.1, 0.1, 0.1, 0.1))\n",
    "\n",
    "fig.suptitle('Tweet distribution', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGxxXGPs1Zk7"
   },
   "source": [
    "The bar graph indicates that [1] which is the Pro sentiment the largest count of tweets and [-1] which is Anti has the lowest count of tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdhFruac_Ilw"
   },
   "source": [
    "* **Popular words distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAvHt7I82bhL"
   },
   "source": [
    "Demonstration of words used in different tweets under each class/sentiment.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "id": "mUjTqUPtYlXF",
    "outputId": "929d4f2c-418a-4153-823e-639f5bd6886c"
   },
   "outputs": [],
   "source": [
    "#word clouds\n",
    "news = train[train['sentiment'] == 2]['message']\n",
    "pro = train[train['sentiment'] == 1]['message']\n",
    "neutral =train[train['sentiment'] == 0]['message']\n",
    "Anti = train[train['sentiment'] ==-1]['message']\n",
    "\n",
    "\n",
    "news = [word for line in news for word in line.split()]\n",
    "pro = [word for line in pro for word in line.split()]\n",
    "neutral = [word for line in neutral for word in line.split()]\n",
    "Anti= [word for line in Anti for word in line.split()]\n",
    "\n",
    "news = WordCloud(\n",
    "    background_color='blue',\n",
    "    max_words=100,\n",
    "    max_font_size=40,\n",
    "    scale=5,\n",
    "    random_state=1,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(news))\n",
    "\n",
    "pro = WordCloud(\n",
    "    background_color='green',\n",
    "    max_words=100,\n",
    "    max_font_size=40,\n",
    "    scale=5,\n",
    "    random_state=1,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(pro))\n",
    "\n",
    "\n",
    "\n",
    "neutral = WordCloud(\n",
    "    background_color='white',\n",
    "    max_words=100,\n",
    "    max_font_size=40,\n",
    "    scale=5,\n",
    "    random_state=1,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(neutral))\n",
    "\n",
    "\n",
    "Anti = WordCloud(\n",
    "    background_color='brown',\n",
    "    max_words=100,\n",
    "    max_font_size=40,\n",
    "    scale=5,\n",
    "    random_state=1,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(Anti))\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize = (20, 10))\n",
    "fig.tight_layout(pad = 0)\n",
    "\n",
    "axs[0, 0].imshow(news)\n",
    "axs[0, 0].set_title('News tweets', fontsize = 20)\n",
    "axs[0, 0].axis('off')\n",
    "\n",
    "axs[0, 1].imshow(pro)\n",
    "axs[0, 1].set_title('Pro tweets', fontsize = 20)\n",
    "axs[0, 1].axis('off')\n",
    "\n",
    "\n",
    "axs[1, 0].imshow(Anti)\n",
    "axs[1, 0].set_title('Anti tweets', fontsize = 20)\n",
    "axs[1, 0].axis('off')\n",
    "\n",
    "axs[1, 1].imshow(neutral)\n",
    "axs[1, 1].set_title('Neutral tweets', fontsize = 20)\n",
    "axs[1, 1].axis('off')\n",
    "\n",
    "plt.savefig('joint_cloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Y7JMqiN1Zk9"
   },
   "source": [
    "Observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dl0KC8711Zk-"
   },
   "source": [
    "1.The most common words used are Global, Climate, Warming,Chnage and rt(which means retweet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuTGnB_V1Zk-"
   },
   "source": [
    "2.The word 'Trump' is a frequently occuring word in all 4 classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6LCDV3_1Zk_"
   },
   "source": [
    "3.Words like 'science' and 'scientist' occur frequently as well which could imply that people are tweeting about scientific studies that support their views on climate change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8qcetU4_jF6"
   },
   "source": [
    "* **Popular hashtags distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCmXvpFv1Zk_"
   },
   "source": [
    "Hashtags are a tool which twittwer uses to orgainize tweets according to their specific categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "kIfNYBTA1Zkt",
    "outputId": "f1db16cb-cdf6-4474-c200-2e10b50f89a2"
   },
   "outputs": [],
   "source": [
    "def hashtag_extract(tweet):   \n",
    "    hashtags = []\n",
    "    \n",
    "    for i in tweet:\n",
    "        ht = re.findall(r\"#(\\w+)\", i)\n",
    "        hashtags.append(ht)\n",
    "        \n",
    "    hashtags = sum(hashtags, [])\n",
    "    frequency = nltk.FreqDist(hashtags)\n",
    "    \n",
    "    hashtag_df = pd.DataFrame({'hashtag': list(frequency.keys()),\n",
    "                       'count': list(frequency.values())})\n",
    "    hashtag_df = hashtag_df.nlargest(15, columns=\"count\")\n",
    "\n",
    "    return hashtag_df\n",
    "\n",
    "# Extracting the hashtags from tweets in each class\n",
    "pro = hashtag_extract(eda_train['message'][eda_train['sentiment'] == 'Pro'])\n",
    "anti = hashtag_extract(eda_train['message'][eda_train['sentiment'] == 'Anti'])\n",
    "neutral = hashtag_extract(eda_train['message'][eda_train['sentiment'] == 'Neutral'])\n",
    "news = hashtag_extract(eda_train['message'][eda_train['sentiment'] == 'News'])\n",
    "\n",
    "pro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "ZgbssNob1ZlA",
    "outputId": "6aec2f38-1244-432f-ca1d-22062cdca48d"
   },
   "outputs": [],
   "source": [
    "sns.barplot(data=pro,y=pro['hashtag'], x=pro['count'], palette=(\"dark\"))\n",
    "plt.title('Popular PRO climate change hashtags')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "JaGS8yv2_uuz",
    "outputId": "690a1c6d-43a8-4651-e80e-91b98cff4993"
   },
   "outputs": [],
   "source": [
    "sns.barplot(y=news['hashtag'], x=news['count'], palette=(\"copper\"))\n",
    "plt.title('Popular NEWS climate change hashtags')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "JFvFxVG21ZlC",
    "outputId": "fc58e20b-1c7a-4866-b249-760b4fecc148"
   },
   "outputs": [],
   "source": [
    "sns.barplot(y=neutral['hashtag'], x=neutral['count'], palette=(\"copper\"))\n",
    "plt.title('Popular NEUTRAL climate change hashtags')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "QCooZDUw1ZlD",
    "outputId": "b04943fc-f5a4-4a60-831a-c5462fcc8df3"
   },
   "outputs": [],
   "source": [
    "sns.barplot(data=anti,y=anti['hashtag'], x=anti['count'], palette=(\"copper\"))\n",
    "plt.title('Popular ANTI climate change hashtags')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqSuNGZg_85e"
   },
   "source": [
    "* **Named entities distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yx-_-CzF1ZlE"
   },
   "outputs": [],
   "source": [
    "#Graph of named entities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5FQDbFVAP6o"
   },
   "source": [
    "* **News channels distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUEkm8THAV03"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nt88rwECAjrl"
   },
   "source": [
    "* **People and places**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xzJgkXsApEs"
   },
   "outputs": [],
   "source": [
    "#code showing tweeters/tweeter mentions bargraphs and cloud per sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7VclK82UJzT"
   },
   "source": [
    "#**4. Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heIrd-pAA_e4"
   },
   "source": [
    "\n",
    "\n",
    "> The cleaned data was fitted in a TFiD and CountVector pipeline with a unigram and bigram analysis and passed through five models and compared their performances. The best performing model was then hyperparamatised to improve its performance. In this way we were able to achieve a well optimized performing model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hvi7AaNDBiAm"
   },
   "source": [
    "##**4.1. Train-test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npzcqw8M1ZlG"
   },
   "outputs": [],
   "source": [
    "X = train['message'].values\n",
    "y = train['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDRmaFOL1464"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
    "                                                  test_size=.05,\n",
    "                                                  random_state=42\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcCyMi1gBwJ0"
   },
   "source": [
    "##**4.2. Train Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVryqf3K1-XC"
   },
   "source": [
    "\n",
    "\n",
    "*  **Vectorizers**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5Q-qW1R2LYW"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define multiple vectorizers to test which one gives us the best accuracy\n",
    "vectorizer_dict = {'CV_1': CountVectorizer(),'TF_1': TfidfVectorizer()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEfca26L2QeI"
   },
   "source": [
    "\n",
    "\n",
    "> **Models**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8j0wyEiB3Nk"
   },
   "outputs": [],
   "source": [
    "model_dict = {'Logistic Regression': LogisticRegression(),\n",
    "              'Naive Bayes': MultinomialNB(),\n",
    "              'LinearSVM': SGDClassifier(),\n",
    "              'Decision Tree': DecisionTreeClassifier(),\n",
    "              'LinearSVC': LinearSVC(),\n",
    "              'SVC': SVC()\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cfErCu82wH6"
   },
   "source": [
    "> **Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vj41kh312PZw",
    "outputId": "a31b8833-c869-4ec3-a768-0e13bb3c60dc"
   },
   "outputs": [],
   "source": [
    "# Run each classifier for each vectorizer\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "classifier_results_dict = defaultdict(list)\n",
    "for vec_name, vectorizer in vectorizer_dict.items():\n",
    "    \n",
    "    X_train_v = vectorizer.fit_transform(X_train)\n",
    "    X_test_v = vectorizer.transform(X_val)\n",
    "    print(vec_name)           # helps keep track of run progress\n",
    "\n",
    "    for mod_name, model in model_dict.items():\n",
    "\n",
    "        # Logging the execution time for each model\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.fit(X_train_v, y_train)\n",
    "        y_pred_v = model.predict(X_test_v)\n",
    "\n",
    "        run_time = time.time()-start_time\n",
    "\n",
    "        precision_v = round(100*precision_score(y_val, y_pred_v,\n",
    "                            average='weighted'), 4)\n",
    "        recall_v = round(100*recall_score(y_val, y_pred_v,\n",
    "                         average='weighted'), 4)\n",
    "        f1_v = round(2*(precision_v*recall_v) / (precision_v+recall_v), 4)\n",
    "\n",
    "        classifier_results_dict['Vectorizer Type'].append(vec_name)\n",
    "        classifier_results_dict['Model Name'].append(mod_name)\n",
    "        classifier_results_dict[('Precision')].append(precision_v)\n",
    "        classifier_results_dict[('Recall')].append(recall_v)\n",
    "        classifier_results_dict[('F1-score')].append(f1_v)\n",
    "        classifier_results_dict[('Execution Time')].append(run_time)\n",
    "        \n",
    "        with open(f\"{mod_name}.plk\", 'wb') as pkl_file:\n",
    "            pickle.dump(mod_name, pkl_file)\n",
    "\n",
    "classifier_results_df = pd.DataFrame(classifier_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyFQQNY4B6D7"
   },
   "source": [
    "#**5. Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrvhwDkG3UO8"
   },
   "source": [
    "##**5.1. Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "fm1AW00o3NGu",
    "outputId": "4709fcd4-dc2f-4c35-dcbf-7c842689b7c1"
   },
   "outputs": [],
   "source": [
    "# Checking result\n",
    "classifier_results_df.sort_values(by='F1-score',\n",
    "                                  ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6yEdeUDC3Xq"
   },
   "source": [
    "##**5.2. Model Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sfrHN07C-1B"
   },
   "source": [
    "* **Models score comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "k1EsjqU23nlR",
    "outputId": "8588ef73-1faa-434b-a190-eb680151354d"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.bar(classifier_results_df, x=\"Model Name\", y=\"F1-score\", color='Vectorizer Type',\n",
    "             barmode='group', height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "2Y_l0xXfBppp",
    "outputId": "d26c5e5e-6b3a-4e0c-b1f7-eb335d3cb471"
   },
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "classifier_results_df.plot(kind='bar',x='Model Name',y='Execution Time',ax=ax)\n",
    "plt.title('Execution Time per Model')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CNuBgtADbkP"
   },
   "source": [
    "* **Hyperparameter tuning of best model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOff5gETDmES"
   },
   "outputs": [],
   "source": [
    "X = train['message'].values\n",
    "y = train['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HypVpB-3xik"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
    "                                                  test_size=.01,\n",
    "                                                  random_state=42\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KenkjIKx32Bh"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             min_df=1, \n",
    "                             max_df=0.9, \n",
    "                             ngram_range=(1, 2))\n",
    "\n",
    "Tfd_SVC_pipe = Pipeline([('vectorizer', vectorizer), ('SVC', SVC())]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2za_6Zry328P",
    "outputId": "c3586044-5f8b-4525-d441-1315c28d25da"
   },
   "outputs": [],
   "source": [
    "Tfd_SVC_pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzLQPu_e38hT"
   },
   "outputs": [],
   "source": [
    "parameters = {'SVC__kernel':('linear', 'rbf'),\n",
    "              'SVC__C':(0.25,1.0),\n",
    "              'SVC__gamma': (1,2)\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnvy0FQHG3V2"
   },
   "outputs": [],
   "source": [
    "\n",
    "clf = GridSearchCV(Tfd_SVC_pipe, parameters)\n",
    "clf.fit(X_train,y_train)\n",
    "print(\"Best Score: \", clf.best_score_)\n",
    "print(\"Best Params: \", clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Brrvn2Xf4H7N"
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AU1s7bf_4Lab",
    "outputId": "c2314779-1fdd-4e77-97e9-970cfa7828c3"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duZcdFVWDhv4"
   },
   "source": [
    "#**Results and Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMnMa-z9Dv4b"
   },
   "outputs": [],
   "source": [
    "test['sentiment'] = clf.predict(test['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "OxqKdBXq4Tqt",
    "outputId": "207ad3c0-d26b-4548-9408-22359cde6bd1"
   },
   "outputs": [],
   "source": [
    "test.drop('message', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Msple9OSD0Y6"
   },
   "outputs": [],
   "source": [
    "test[['tweetid','sentiment']].to_csv('submissionmpho.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k98RVIwIEHKU"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('submissionmpho.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "tVdlIRJ74e57",
    "outputId": "2d8606df-9df0-44fc-bafe-0f3ea4108210"
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdMnbsmoDzXS"
   },
   "source": [
    "#**Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-lS9DBxEI32"
   },
   "source": [
    "Conclude on results, how the prediction performed, what can be improved or whatever you see fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITubC0JlEAlO"
   },
   "source": [
    "#**References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTpUOZWBEaKX"
   },
   "source": [
    "References of sources of information (can be informal)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Climate Sentiment Analysis Final.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
